{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df1832d-975f-4773-bdcf-343c8cf18065",
   "metadata": {},
   "source": [
    "# Morning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41e9528-e3e6-453e-a09c-73ddf86d58e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 1 person, 8 cars, 5 motorcycles, 1 truck, 1 potted plant, 312.0ms\n",
      "Speed: 8.3ms preprocess, 312.0ms inference, 3.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 5 motorcycles, 1 potted plant, 289.7ms\n",
      "Speed: 6.0ms preprocess, 289.7ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 4 motorcycles, 1 potted plant, 284.0ms\n",
      "Speed: 8.1ms preprocess, 284.0ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 4 motorcycles, 1 potted plant, 296.8ms\n",
      "Speed: 21.4ms preprocess, 296.8ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 6 cars, 4 motorcycles, 1 potted plant, 228.1ms\n",
      "Speed: 6.1ms preprocess, 228.1ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 5 motorcycles, 1 potted plant, 223.7ms\n",
      "Speed: 4.5ms preprocess, 223.7ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 5 motorcycles, 1 potted plant, 235.5ms\n",
      "Speed: 3.7ms preprocess, 235.5ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 5 motorcycles, 1 potted plant, 238.7ms\n",
      "Speed: 3.8ms preprocess, 238.7ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 4 cars, 5 motorcycles, 1 potted plant, 221.8ms\n",
      "Speed: 3.7ms preprocess, 221.8ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 6 motorcycles, 1 potted plant, 221.9ms\n",
      "Speed: 3.9ms preprocess, 221.9ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 5 motorcycles, 1 potted plant, 232.8ms\n",
      "Speed: 5.9ms preprocess, 232.8ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 6 motorcycles, 1 potted plant, 234.7ms\n",
      "Speed: 5.0ms preprocess, 234.7ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 6 motorcycles, 1 potted plant, 218.8ms\n",
      "Speed: 4.4ms preprocess, 218.8ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 6 cars, 6 motorcycles, 1 potted plant, 222.1ms\n",
      "Speed: 3.5ms preprocess, 222.1ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 5 motorcycles, 1 potted plant, 212.6ms\n",
      "Speed: 2.7ms preprocess, 212.6ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 cars, 6 motorcycles, 1 potted plant, 225.7ms\n",
      "Speed: 4.2ms preprocess, 225.7ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 6 cars, 5 motorcycles, 1 potted plant, 223.8ms\n",
      "Speed: 4.6ms preprocess, 223.8ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 8 cars, 4 motorcycles, 1 potted plant, 223.3ms\n",
      "Speed: 3.6ms preprocess, 223.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 6 cars, 5 motorcycles, 1 potted plant, 219.9ms\n",
      "Speed: 4.0ms preprocess, 219.9ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 7 cars, 6 motorcycles, 1 potted plant, 217.0ms\n",
      "Speed: 4.5ms preprocess, 217.0ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 7 cars, 7 motorcycles, 1 potted plant, 221.2ms\n",
      "Speed: 3.5ms preprocess, 221.2ms inference, 2.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 6 cars, 7 motorcycles, 1 potted plant, 214.8ms\n",
      "Speed: 4.6ms preprocess, 214.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 7 cars, 6 motorcycles, 1 potted plant, 214.9ms\n",
      "Speed: 3.2ms preprocess, 214.9ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 6 motorcycles, 1 potted plant, 233.4ms\n",
      "Speed: 4.1ms preprocess, 233.4ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 9 cars, 6 motorcycles, 236.0ms\n",
      "Speed: 5.3ms preprocess, 236.0ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 5 cars, 5 motorcycles, 1 potted plant, 211.1ms\n",
      "Speed: 2.8ms preprocess, 211.1ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 4 cars, 6 motorcycles, 213.9ms\n",
      "Speed: 5.6ms preprocess, 213.9ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 5 motorcycles, 1 potted plant, 220.1ms\n",
      "Speed: 3.4ms preprocess, 220.1ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 5 cars, 4 motorcycles, 1 potted plant, 223.7ms\n",
      "Speed: 4.1ms preprocess, 223.7ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 5 cars, 4 motorcycles, 1 potted plant, 229.4ms\n",
      "Speed: 4.0ms preprocess, 229.4ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 4 motorcycles, 1 potted plant, 216.7ms\n",
      "Speed: 3.0ms preprocess, 216.7ms inference, 2.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 4 cars, 3 motorcycles, 1 potted plant, 225.3ms\n",
      "Speed: 3.0ms preprocess, 225.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 4 cars, 3 motorcycles, 1 potted plant, 234.0ms\n",
      "Speed: 4.1ms preprocess, 234.0ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 persons, 4 cars, 4 motorcycles, 1 potted plant, 220.9ms\n",
      "Speed: 2.9ms preprocess, 220.9ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 persons, 7 cars, 3 motorcycles, 1 potted plant, 217.2ms\n",
      "Speed: 3.8ms preprocess, 217.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 persons, 5 cars, 3 motorcycles, 1 potted plant, 223.4ms\n",
      "Speed: 3.9ms preprocess, 223.4ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 persons, 7 cars, 3 motorcycles, 1 potted plant, 222.3ms\n",
      "Speed: 3.9ms preprocess, 222.3ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 7 cars, 3 motorcycles, 1 potted plant, 218.5ms\n",
      "Speed: 7.7ms preprocess, 218.5ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 7 cars, 3 motorcycles, 1 potted plant, 220.2ms\n",
      "Speed: 5.5ms preprocess, 220.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 6 cars, 3 motorcycles, 1 potted plant, 210.7ms\n",
      "Speed: 2.8ms preprocess, 210.7ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 3 motorcycles, 1 potted plant, 215.9ms\n",
      "Speed: 5.3ms preprocess, 215.9ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 3 motorcycles, 1 potted plant, 248.2ms\n",
      "Speed: 3.5ms preprocess, 248.2ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 4 cars, 3 motorcycles, 1 potted plant, 243.8ms\n",
      "Speed: 4.9ms preprocess, 243.8ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 3 motorcycles, 1 potted plant, 216.0ms\n",
      "Speed: 5.0ms preprocess, 216.0ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 4 cars, 3 motorcycles, 1 potted plant, 221.1ms\n",
      "Speed: 4.7ms preprocess, 221.1ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 3 motorcycles, 1 potted plant, 219.8ms\n",
      "Speed: 4.2ms preprocess, 219.8ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 4 cars, 3 motorcycles, 1 potted plant, 218.7ms\n",
      "Speed: 5.1ms preprocess, 218.7ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 4 cars, 3 motorcycles, 1 potted plant, 215.5ms\n",
      "Speed: 4.5ms preprocess, 215.5ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 3 motorcycles, 1 potted plant, 215.7ms\n",
      "Speed: 3.8ms preprocess, 215.7ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 3 motorcycles, 1 potted plant, 230.4ms\n",
      "Speed: 4.1ms preprocess, 230.4ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 4 motorcycles, 1 potted plant, 222.7ms\n",
      "Speed: 5.2ms preprocess, 222.7ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 3 motorcycles, 1 potted plant, 215.6ms\n",
      "Speed: 4.1ms preprocess, 215.6ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 2 motorcycles, 1 potted plant, 212.4ms\n",
      "Speed: 5.3ms preprocess, 212.4ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 3 motorcycles, 1 potted plant, 212.6ms\n",
      "Speed: 4.7ms preprocess, 212.6ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 6 cars, 2 motorcycles, 1 potted plant, 217.9ms\n",
      "Speed: 4.5ms preprocess, 217.9ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 6 cars, 3 motorcycles, 214.2ms\n",
      "Speed: 5.9ms preprocess, 214.2ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 2 motorcycles, 366.0ms\n",
      "Speed: 3.7ms preprocess, 366.0ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 3 motorcycles, 1 bus, 234.1ms\n",
      "Speed: 3.5ms preprocess, 234.1ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 5 motorcycles, 1 bus, 1 potted plant, 222.9ms\n",
      "Speed: 4.4ms preprocess, 222.9ms inference, 2.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 7 cars, 5 motorcycles, 1 bus, 1 potted plant, 227.2ms\n",
      "Speed: 3.7ms preprocess, 227.2ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 5 cars, 5 motorcycles, 1 bus, 1 potted plant, 247.2ms\n",
      "Speed: 4.6ms preprocess, 247.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 6 motorcycles, 1 bus, 1 potted plant, 270.5ms\n",
      "Speed: 4.4ms preprocess, 270.5ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 5 motorcycles, 1 bus, 1 potted plant, 244.3ms\n",
      "Speed: 7.8ms preprocess, 244.3ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 4 motorcycles, 1 potted plant, 236.8ms\n",
      "Speed: 5.1ms preprocess, 236.8ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 4 motorcycles, 1 potted plant, 226.9ms\n",
      "Speed: 5.5ms preprocess, 226.9ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 3 motorcycles, 1 potted plant, 232.6ms\n",
      "Speed: 7.7ms preprocess, 232.6ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 8 motorcycles, 236.8ms\n",
      "Speed: 5.5ms preprocess, 236.8ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 4 cars, 6 motorcycles, 227.9ms\n",
      "Speed: 6.3ms preprocess, 227.9ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 7 motorcycles, 227.2ms\n",
      "Speed: 5.4ms preprocess, 227.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 4 cars, 5 motorcycles, 231.5ms\n",
      "Speed: 5.5ms preprocess, 231.5ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 4 cars, 5 motorcycles, 243.5ms\n",
      "Speed: 6.5ms preprocess, 243.5ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 6 motorcycles, 231.4ms\n",
      "Speed: 7.6ms preprocess, 231.4ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 6 cars, 6 motorcycles, 261.6ms\n",
      "Speed: 5.7ms preprocess, 261.6ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 6 cars, 6 motorcycles, 1 potted plant, 242.3ms\n",
      "Speed: 5.6ms preprocess, 242.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 8 motorcycles, 247.0ms\n",
      "Speed: 8.5ms preprocess, 247.0ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 5 cars, 7 motorcycles, 1 potted plant, 243.6ms\n",
      "Speed: 5.9ms preprocess, 243.6ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 7 motorcycles, 1 potted plant, 236.4ms\n",
      "Speed: 6.2ms preprocess, 236.4ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 4 cars, 7 motorcycles, 1 potted plant, 250.8ms\n",
      "Speed: 6.1ms preprocess, 250.8ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 4 cars, 7 motorcycles, 1 potted plant, 239.7ms\n",
      "Speed: 8.2ms preprocess, 239.7ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 4 cars, 6 motorcycles, 1 potted plant, 266.1ms\n",
      "Speed: 8.0ms preprocess, 266.1ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 5 cars, 7 motorcycles, 1 potted plant, 333.7ms\n",
      "Speed: 7.3ms preprocess, 333.7ms inference, 4.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 5 cars, 7 motorcycles, 1 potted plant, 276.5ms\n",
      "Speed: 8.2ms preprocess, 276.5ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 persons, 4 cars, 7 motorcycles, 242.2ms\n",
      "Speed: 6.3ms preprocess, 242.2ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 4 cars, 6 motorcycles, 250.2ms\n",
      "Speed: 8.0ms preprocess, 250.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 4 cars, 6 motorcycles, 241.8ms\n",
      "Speed: 6.4ms preprocess, 241.8ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 3 cars, 6 motorcycles, 248.0ms\n",
      "Speed: 5.4ms preprocess, 248.0ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 3 cars, 6 motorcycles, 1 potted plant, 264.0ms\n",
      "Speed: 9.3ms preprocess, 264.0ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 persons, 3 cars, 6 motorcycles, 1 potted plant, 242.4ms\n",
      "Speed: 7.3ms preprocess, 242.4ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 persons, 3 cars, 6 motorcycles, 1 potted plant, 242.1ms\n",
      "Speed: 6.5ms preprocess, 242.1ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 persons, 3 cars, 6 motorcycles, 1 potted plant, 249.5ms\n",
      "Speed: 7.2ms preprocess, 249.5ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 4 cars, 6 motorcycles, 1 potted plant, 284.0ms\n",
      "Speed: 19.1ms preprocess, 284.0ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 persons, 4 cars, 5 motorcycles, 1 potted plant, 243.3ms\n",
      "Speed: 9.6ms preprocess, 243.3ms inference, 3.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 3 cars, 6 motorcycles, 243.4ms\n",
      "Speed: 5.9ms preprocess, 243.4ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 2 cars, 7 motorcycles, 1 potted plant, 256.9ms\n",
      "Speed: 5.4ms preprocess, 256.9ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 2 cars, 5 motorcycles, 1 potted plant, 242.2ms\n",
      "Speed: 7.4ms preprocess, 242.2ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 2 cars, 4 motorcycles, 1 potted plant, 239.2ms\n",
      "Speed: 5.1ms preprocess, 239.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 2 cars, 4 motorcycles, 1 potted plant, 238.4ms\n",
      "Speed: 5.6ms preprocess, 238.4ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 2 cars, 4 motorcycles, 1 potted plant, 242.7ms\n",
      "Speed: 5.5ms preprocess, 242.7ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 3 cars, 5 motorcycles, 1 potted plant, 244.9ms\n",
      "Speed: 6.9ms preprocess, 244.9ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 3 cars, 5 motorcycles, 1 potted plant, 250.5ms\n",
      "Speed: 6.3ms preprocess, 250.5ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 3 cars, 5 motorcycles, 1 potted plant, 269.1ms\n",
      "Speed: 6.3ms preprocess, 269.1ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 3 cars, 5 motorcycles, 1 potted plant, 229.0ms\n",
      "Speed: 7.2ms preprocess, 229.0ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 4 cars, 5 motorcycles, 1 potted plant, 272.8ms\n",
      "Speed: 10.0ms preprocess, 272.8ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 4 cars, 5 motorcycles, 1 potted plant, 262.2ms\n",
      "Speed: 6.7ms preprocess, 262.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 5 motorcycles, 1 potted plant, 278.3ms\n",
      "Speed: 8.5ms preprocess, 278.3ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 3 cars, 8 motorcycles, 1 potted plant, 283.9ms\n",
      "Speed: 5.1ms preprocess, 283.9ms inference, 4.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m1020\u001b[39m, \u001b[38;5;241m500\u001b[39m))\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# ✅ YOLO Detection\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m a \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     63\u001b[0m px \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(a)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\model.py:560\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:261\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 261\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:145\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    140\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    141\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    144\u001b[0m )\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\autobackend.py:557\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 557\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:114\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:132\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:153\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 153\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    154\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\modules\\block.py:239\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 239\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\modules\\block.py:239\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 239\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\modules\\block.py:348\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\modules\\conv.py:55\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution and activation without batch normalization.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from tracker import *\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ✅ Load YOLO model\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "# ✅ Object classes\n",
    "class_list = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "    \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "    \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\",\n",
    "    \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\n",
    "    \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
    "    \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
    "    \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"TV\", \"laptop\",\n",
    "    \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "    \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "# ✅ Initialize tracker\n",
    "tracker = Tracker()\n",
    "cap = cv2.VideoCapture('bus_stand_morning.MOV')\n",
    "\n",
    "# ✅ CSV File Setup\n",
    "csv_filename = \"12vehicles_summary.csv\"\n",
    "\n",
    "# ✅ Create CSV file if it doesn't exist\n",
    "if not os.path.exists(csv_filename):\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Timestamp\", \"Cars\", \"Buses\", \"Trucks\", \"Motorcycles\"])\n",
    "\n",
    "# ✅ Define reference lines\n",
    "red_line_y = 355\n",
    "blue_line_y = 385\n",
    "offset = 7  # Allow slight variation due to tracking jitter\n",
    "\n",
    "# ✅ Define vehicle classes to track\n",
    "vehicle_classes = [\"car\", \"bus\", \"truck\", \"motorcycle\"]\n",
    "vehicle_counts = {\"car\": 0, \"bus\": 0, \"truck\": 0, \"motorcycle\": 0}\n",
    "\n",
    "# ✅ Store vehicle IDs that crossed the red line first\n",
    "crossed_red = {}\n",
    "counter_down = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (1020, 500))\n",
    "\n",
    "    # ✅ YOLO Detection\n",
    "    results = model.predict(frame)\n",
    "    a = results[0].boxes.data.detach().cpu().numpy()\n",
    "    px = pd.DataFrame(a).astype(\"float\")\n",
    "\n",
    "    # ✅ Store detections\n",
    "    detections = []\n",
    "    detected_vehicles = {}\n",
    "\n",
    "    for _, row in px.iterrows():\n",
    "        x1, y1, x2, y2 = int(row[0]), int(row[1]), int(row[2]), int(row[3])\n",
    "        d = int(row[5])\n",
    "        vehicle_type = class_list[d]\n",
    "\n",
    "        if vehicle_type in vehicle_classes:\n",
    "            detections.append([x1, y1, x2, y2])\n",
    "            detected_vehicles[(x1, y1, x2, y2)] = vehicle_type\n",
    "\n",
    "    # ✅ Ensure 'detections' is not empty\n",
    "    bbox_id = tracker.update(np.array(detections)) if detections else []\n",
    "\n",
    "    # ✅ Process ALL detected vehicles\n",
    "    for bbox in bbox_id:\n",
    "        x3, y3, x4, y4, vehicle_id = bbox\n",
    "        cx, cy = (x3 + x4) // 2, (y3 + y4) // 2\n",
    "\n",
    "        vehicle_type = detected_vehicles.get((x3, y3, x4, y4), \"unknown\")\n",
    "\n",
    "        # ✅ Check if the vehicle crosses the RED line first\n",
    "        if red_line_y - offset < cy < red_line_y + offset:\n",
    "            crossed_red[vehicle_id] = (cy, vehicle_type)\n",
    "\n",
    "        # ✅ Only consider vehicles that crossed red before reaching blue\n",
    "        if vehicle_id in crossed_red and blue_line_y - offset < cy < blue_line_y + offset:\n",
    "            _, v_type = crossed_red[vehicle_id]\n",
    "\n",
    "            # ✅ Draw circle at the blue line for valid vehicles\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"{v_type}-{vehicle_id}\", (cx, cy), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "            # ✅ Add to counter only once\n",
    "            if vehicle_id not in counter_down:\n",
    "                counter_down.append(vehicle_id)\n",
    "                vehicle_counts[v_type] += 1\n",
    "\n",
    "    # ✅ Draw reference lines\n",
    "    cv2.line(frame, (260, red_line_y), (724, red_line_y), (0, 0, 255), 3)\n",
    "    cv2.putText(frame, 'red line', (260, red_line_y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.line(frame, (200, blue_line_y), (779, blue_line_y), (255, 0, 0), 3)\n",
    "    cv2.putText(frame, 'blue line', (200, blue_line_y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # ✅ Display vehicle count\n",
    "    cv2.putText(frame, f\"Cars: {vehicle_counts['car']}  Buses: {vehicle_counts['bus']}  Trucks: {vehicle_counts['truck']}  Motorcycles: {vehicle_counts['motorcycle']}\",\n",
    "                (60, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"frames\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# ✅ Log the final vehicle count when the video ends\n",
    "timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "number_of_lanes=2\n",
    "# ✅ Calculate light duration using the given formula\n",
    "light_duration = ((vehicle_counts[\"car\"] * 2) + \n",
    "                  (vehicle_counts[\"motorcycle\"] * 1.8) + \n",
    "                  ((vehicle_counts[\"bus\"] + vehicle_counts[\"truck\"]) * 3.5)) / number_of_lanes\n",
    "with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([timestamp, vehicle_counts[\"car\"], vehicle_counts[\"bus\"], vehicle_counts[\"truck\"], vehicle_counts[\"motorcycle\"], round(light_duration, 2)])\n",
    "\n",
    "print(f\"✅ Final vehicle count saved to CSV: {vehicle_counts}, Light Duration: {round(light_duration, 2)} sec\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e87f7-c4a3-40da-afc5-14b30dc48eb6",
   "metadata": {},
   "source": [
    "# Rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4f5045-c0c5-4302-a838-6eabe0add724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 1 person, 6 cars, 2 motorcycles, 1 traffic light, 225.6ms\n",
      "Speed: 6.3ms preprocess, 225.6ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 motorcycle, 1 traffic light, 266.3ms\n",
      "Speed: 3.9ms preprocess, 266.3ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 2 motorcycles, 1 traffic light, 276.4ms\n",
      "Speed: 3.8ms preprocess, 276.4ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 1 motorcycle, 1 truck, 1 traffic light, 247.4ms\n",
      "Speed: 5.6ms preprocess, 247.4ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 1 motorcycle, 1 truck, 1 traffic light, 266.0ms\n",
      "Speed: 7.7ms preprocess, 266.0ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 1 truck, 1 traffic light, 225.2ms\n",
      "Speed: 4.7ms preprocess, 225.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 1 motorcycle, 1 truck, 1 traffic light, 235.0ms\n",
      "Speed: 3.1ms preprocess, 235.0ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 1 motorcycle, 1 traffic light, 227.5ms\n",
      "Speed: 4.5ms preprocess, 227.5ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 244.2ms\n",
      "Speed: 3.7ms preprocess, 244.2ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 1 truck, 240.3ms\n",
      "Speed: 4.7ms preprocess, 240.3ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 1 truck, 273.8ms\n",
      "Speed: 4.8ms preprocess, 273.8ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 1 truck, 335.9ms\n",
      "Speed: 6.0ms preprocess, 335.9ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 321.6ms\n",
      "Speed: 10.2ms preprocess, 321.6ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 308.6ms\n",
      "Speed: 5.9ms preprocess, 308.6ms inference, 3.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 306.7ms\n",
      "Speed: 6.3ms preprocess, 306.7ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 292.1ms\n",
      "Speed: 5.4ms preprocess, 292.1ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 313.3ms\n",
      "Speed: 6.6ms preprocess, 313.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 277.5ms\n",
      "Speed: 8.1ms preprocess, 277.5ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 1 traffic light, 255.2ms\n",
      "Speed: 7.5ms preprocess, 255.2ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 1 traffic light, 262.8ms\n",
      "Speed: 5.2ms preprocess, 262.8ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 1 traffic light, 257.8ms\n",
      "Speed: 7.3ms preprocess, 257.8ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 2 traffic lights, 286.7ms\n",
      "Speed: 6.2ms preprocess, 286.7ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 1 traffic light, 250.7ms\n",
      "Speed: 5.1ms preprocess, 250.7ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 1 traffic light, 250.7ms\n",
      "Speed: 5.7ms preprocess, 250.7ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 1 motorcycle, 2 traffic lights, 248.4ms\n",
      "Speed: 5.2ms preprocess, 248.4ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 1 motorcycle, 1 traffic light, 251.4ms\n",
      "Speed: 6.8ms preprocess, 251.4ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 motorcycle, 2 traffic lights, 249.3ms\n",
      "Speed: 5.5ms preprocess, 249.3ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 1 motorcycle, 1 traffic light, 251.2ms\n",
      "Speed: 6.4ms preprocess, 251.2ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 1 motorcycle, 1 truck, 1 traffic light, 271.2ms\n",
      "Speed: 5.5ms preprocess, 271.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 8 cars, 1 bus, 1 traffic light, 316.3ms\n",
      "Speed: 5.5ms preprocess, 316.3ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 8 cars, 1 bus, 1 truck, 1 traffic light, 303.7ms\n",
      "Speed: 6.6ms preprocess, 303.7ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 6 cars, 1 bus, 1 traffic light, 284.7ms\n",
      "Speed: 5.7ms preprocess, 284.7ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 1 bus, 1 traffic light, 299.4ms\n",
      "Speed: 7.8ms preprocess, 299.4ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 6 cars, 1 bus, 1 traffic light, 265.0ms\n",
      "Speed: 8.7ms preprocess, 265.0ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 7 cars, 1 bus, 1 traffic light, 262.9ms\n",
      "Speed: 10.2ms preprocess, 262.9ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 8 cars, 1 bus, 1 traffic light, 293.1ms\n",
      "Speed: 11.5ms preprocess, 293.1ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 cars, 1 bus, 1 traffic light, 242.3ms\n",
      "Speed: 7.1ms preprocess, 242.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 cars, 1 bus, 1 traffic light, 235.4ms\n",
      "Speed: 6.7ms preprocess, 235.4ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 bus, 1 traffic light, 525.5ms\n",
      "Speed: 9.5ms preprocess, 525.5ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 bus, 238.5ms\n",
      "Speed: 5.7ms preprocess, 238.5ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 cars, 1 bus, 1 traffic light, 235.3ms\n",
      "Speed: 5.4ms preprocess, 235.3ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 bus, 1 traffic light, 272.2ms\n",
      "Speed: 8.4ms preprocess, 272.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 bus, 230.7ms\n",
      "Speed: 9.8ms preprocess, 230.7ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 bus, 1 traffic light, 237.7ms\n",
      "Speed: 6.2ms preprocess, 237.7ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 bus, 245.2ms\n",
      "Speed: 6.7ms preprocess, 245.2ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 bus, 1 truck, 234.8ms\n",
      "Speed: 6.8ms preprocess, 234.8ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 cars, 1 bus, 1 truck, 229.7ms\n",
      "Speed: 5.1ms preprocess, 229.7ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 cars, 1 bus, 1 truck, 1 traffic light, 241.8ms\n",
      "Speed: 5.6ms preprocess, 241.8ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 cars, 1 bus, 1 truck, 1 traffic light, 242.6ms\n",
      "Speed: 4.8ms preprocess, 242.6ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 10 cars, 1 bus, 1 truck, 235.7ms\n",
      "Speed: 5.3ms preprocess, 235.7ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 cars, 1 bus, 1 truck, 1 traffic light, 277.5ms\n",
      "Speed: 5.0ms preprocess, 277.5ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 8 cars, 1 bus, 1 truck, 1 traffic light, 270.7ms\n",
      "Speed: 6.4ms preprocess, 270.7ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 6 cars, 1 bus, 1 truck, 1 traffic light, 227.2ms\n",
      "Speed: 5.5ms preprocess, 227.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 1 bus, 1 truck, 225.9ms\n",
      "Speed: 5.6ms preprocess, 225.9ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 1 bus, 1 train, 1 traffic light, 233.9ms\n",
      "Speed: 5.4ms preprocess, 233.9ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 5 cars, 1 bus, 1 truck, 1 traffic light, 233.0ms\n",
      "Speed: 7.8ms preprocess, 233.0ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 7 cars, 1 bus, 1 traffic light, 256.1ms\n",
      "Speed: 5.5ms preprocess, 256.1ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 7 cars, 1 bus, 1 truck, 1 traffic light, 227.8ms\n",
      "Speed: 6.6ms preprocess, 227.8ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 7 cars, 2 buss, 1 truck, 234.0ms\n",
      "Speed: 6.3ms preprocess, 234.0ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 1 bus, 1 traffic light, 229.2ms\n",
      "Speed: 5.1ms preprocess, 229.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 1 motorcycle, 2 buss, 1 truck, 225.5ms\n",
      "Speed: 6.4ms preprocess, 225.5ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 1 motorcycle, 2 buss, 1 truck, 229.8ms\n",
      "Speed: 4.5ms preprocess, 229.8ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 4 cars, 1 motorcycle, 2 buss, 1 truck, 227.1ms\n",
      "Speed: 4.6ms preprocess, 227.1ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 cars, 1 motorcycle, 1 bus, 1 truck, 1 traffic light, 253.9ms\n",
      "Speed: 10.2ms preprocess, 253.9ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 cars, 1 motorcycle, 1 bus, 1 truck, 234.0ms\n",
      "Speed: 6.1ms preprocess, 234.0ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 8 cars, 1 motorcycle, 1 bus, 1 traffic light, 241.2ms\n",
      "Speed: 6.0ms preprocess, 241.2ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 7 cars, 1 bus, 264.0ms\n",
      "Speed: 8.7ms preprocess, 264.0ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 1 bus, 1 traffic light, 231.0ms\n",
      "Speed: 6.8ms preprocess, 231.0ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 9 cars, 1 bus, 1 traffic light, 229.4ms\n",
      "Speed: 5.5ms preprocess, 229.4ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 7 cars, 1 bus, 1 traffic light, 238.8ms\n",
      "Speed: 7.7ms preprocess, 238.8ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 8 cars, 1 bus, 1 traffic light, 227.3ms\n",
      "Speed: 4.2ms preprocess, 227.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 8 cars, 1 bus, 1 truck, 1 traffic light, 249.0ms\n",
      "Speed: 4.5ms preprocess, 249.0ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 bus, 1 truck, 1 traffic light, 251.3ms\n",
      "Speed: 5.6ms preprocess, 251.3ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 7 cars, 1 bus, 231.0ms\n",
      "Speed: 7.0ms preprocess, 231.0ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 6 cars, 1 bus, 1 traffic light, 225.3ms\n",
      "Speed: 5.5ms preprocess, 225.3ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 5 cars, 1 bus, 1 traffic light, 237.5ms\n",
      "Speed: 4.8ms preprocess, 237.5ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 5 cars, 1 traffic light, 244.3ms\n",
      "Speed: 5.1ms preprocess, 244.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 8 cars, 1 bus, 1 traffic light, 226.2ms\n",
      "Speed: 5.4ms preprocess, 226.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 7 cars, 231.8ms\n",
      "Speed: 7.4ms preprocess, 231.8ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 8 cars, 1 bus, 1 traffic light, 254.5ms\n",
      "Speed: 7.7ms preprocess, 254.5ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 10 cars, 1 bus, 1 traffic light, 231.6ms\n",
      "Speed: 5.5ms preprocess, 231.6ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 8 cars, 227.2ms\n",
      "Speed: 5.7ms preprocess, 227.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 7 cars, 1 traffic light, 231.6ms\n",
      "Speed: 5.8ms preprocess, 231.6ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 7 cars, 1 traffic light, 237.6ms\n",
      "Speed: 5.0ms preprocess, 237.6ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 cars, 1 traffic light, 234.7ms\n",
      "Speed: 4.6ms preprocess, 234.7ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 1 traffic light, 225.7ms\n",
      "Speed: 5.7ms preprocess, 225.7ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 traffic light, 245.1ms\n",
      "Speed: 4.7ms preprocess, 245.1ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 8 cars, 1 truck, 1 traffic light, 248.2ms\n",
      "Speed: 6.2ms preprocess, 248.2ms inference, 4.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 cars, 1 bus, 1 traffic light, 274.0ms\n",
      "Speed: 7.2ms preprocess, 274.0ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 10 cars, 1 bus, 1 traffic light, 242.9ms\n",
      "Speed: 9.2ms preprocess, 242.9ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 cars, 1 bus, 1 traffic light, 230.1ms\n",
      "Speed: 6.1ms preprocess, 230.1ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 10 cars, 1 bus, 1 traffic light, 238.5ms\n",
      "Speed: 5.3ms preprocess, 238.5ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 10 cars, 1 bus, 1 traffic light, 231.3ms\n",
      "Speed: 5.7ms preprocess, 231.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 cars, 1 traffic light, 230.3ms\n",
      "Speed: 5.3ms preprocess, 230.3ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 7 cars, 1 bus, 1 traffic light, 251.2ms\n",
      "Speed: 5.4ms preprocess, 251.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 8 cars, 1 traffic light, 233.2ms\n",
      "Speed: 5.9ms preprocess, 233.2ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 7 cars, 1 bus, 1 traffic light, 231.9ms\n",
      "Speed: 6.4ms preprocess, 231.9ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 1 bus, 1 traffic light, 240.4ms\n",
      "Speed: 6.6ms preprocess, 240.4ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 7 cars, 1 traffic light, 246.6ms\n",
      "Speed: 5.1ms preprocess, 246.6ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 9 cars, 1 traffic light, 229.0ms\n",
      "Speed: 4.7ms preprocess, 229.0ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 cars, 1 traffic light, 259.2ms\n",
      "Speed: 5.9ms preprocess, 259.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 cars, 1 traffic light, 296.2ms\n",
      "Speed: 6.5ms preprocess, 296.2ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 cars, 1 traffic light, 279.9ms\n",
      "Speed: 5.9ms preprocess, 279.9ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 8 cars, 1 traffic light, 340.9ms\n",
      "Speed: 9.0ms preprocess, 340.9ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 1 traffic light, 313.1ms\n",
      "Speed: 6.3ms preprocess, 313.1ms inference, 3.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 traffic light, 317.5ms\n",
      "Speed: 9.4ms preprocess, 317.5ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 cars, 1 traffic light, 274.2ms\n",
      "Speed: 5.8ms preprocess, 274.2ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 traffic light, 270.7ms\n",
      "Speed: 10.2ms preprocess, 270.7ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 9 cars, 1 traffic light, 249.0ms\n",
      "Speed: 5.5ms preprocess, 249.0ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 9 cars, 245.9ms\n",
      "Speed: 7.4ms preprocess, 245.9ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 8 cars, 258.8ms\n",
      "Speed: 9.1ms preprocess, 258.8ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m1020\u001b[39m, \u001b[38;5;241m500\u001b[39m))\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# ✅ YOLO Detection\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m a \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     63\u001b[0m px \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(a)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\model.py:560\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:261\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 261\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:145\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    140\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    141\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    144\u001b[0m )\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\autobackend.py:557\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 557\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:114\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:132\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:153\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 153\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    154\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\modules\\block.py:188\u001b[0m, in \u001b[0;36mSPPF.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through Ghost Convolution block.\"\"\"\u001b[39;00m\n\u001b[0;32m    187\u001b[0m y \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)]\n\u001b[1;32m--> 188\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\modules\\block.py:188\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through Ghost Convolution block.\"\"\"\u001b[39;00m\n\u001b[0;32m    187\u001b[0m y \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)]\n\u001b[1;32m--> 188\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from tracker import *\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ✅ Load YOLO model\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "# ✅ Object classes\n",
    "class_list = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "    \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "    \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\",\n",
    "    \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\n",
    "    \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
    "    \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
    "    \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"TV\", \"laptop\",\n",
    "    \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "    \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "# ✅ Initialize tracker\n",
    "tracker = Tracker()\n",
    "cap = cv2.VideoCapture('bus_stand_rain_noon.mp4')\n",
    "\n",
    "# ✅ CSV File Setup\n",
    "csv_filename = \"12vehicles_summary.csv\"\n",
    "\n",
    "# ✅ Create CSV file if it doesn't exist\n",
    "if not os.path.exists(csv_filename):\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Timestamp\", \"Cars\", \"Buses\", \"Trucks\", \"Motorcycles\"])\n",
    "\n",
    "# ✅ Define reference lines\n",
    "red_line_y = 318\n",
    "blue_line_y = 337\n",
    "offset = 7  # Allow slight variation due to tracking jitter\n",
    "\n",
    "# ✅ Define vehicle classes to track\n",
    "vehicle_classes = [\"car\", \"bus\", \"truck\", \"motorcycle\"]\n",
    "vehicle_counts = {\"car\": 0, \"bus\": 0, \"truck\": 0, \"motorcycle\": 0}\n",
    "\n",
    "# ✅ Store vehicle IDs that crossed the red line first\n",
    "crossed_red = {}\n",
    "counter_down = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (1020, 500))\n",
    "\n",
    "    # ✅ YOLO Detection\n",
    "    results = model.predict(frame)\n",
    "    a = results[0].boxes.data.detach().cpu().numpy()\n",
    "    px = pd.DataFrame(a).astype(\"float\")\n",
    "\n",
    "    # ✅ Store detections\n",
    "    detections = []\n",
    "    detected_vehicles = {}\n",
    "\n",
    "    for _, row in px.iterrows():\n",
    "        x1, y1, x2, y2 = int(row[0]), int(row[1]), int(row[2]), int(row[3])\n",
    "        d = int(row[5])\n",
    "        vehicle_type = class_list[d]\n",
    "\n",
    "        if vehicle_type in vehicle_classes:\n",
    "            detections.append([x1, y1, x2, y2])\n",
    "            detected_vehicles[(x1, y1, x2, y2)] = vehicle_type\n",
    "\n",
    "    # ✅ Ensure 'detections' is not empty\n",
    "    bbox_id = tracker.update(np.array(detections)) if detections else []\n",
    "\n",
    "    # ✅ Process ALL detected vehicles\n",
    "    for bbox in bbox_id:\n",
    "        x3, y3, x4, y4, vehicle_id = bbox\n",
    "        cx, cy = (x3 + x4) // 2, (y3 + y4) // 2\n",
    "\n",
    "        vehicle_type = detected_vehicles.get((x3, y3, x4, y4), \"unknown\")\n",
    "\n",
    "        # ✅ Check if the vehicle crosses the RED line first\n",
    "        if red_line_y - offset < cy < red_line_y + offset:\n",
    "            crossed_red[vehicle_id] = (cy, vehicle_type)\n",
    "\n",
    "        # ✅ Only consider vehicles that crossed red before reaching blue\n",
    "        if vehicle_id in crossed_red and blue_line_y - offset < cy < blue_line_y + offset:\n",
    "            _, v_type = crossed_red[vehicle_id]\n",
    "\n",
    "            # ✅ Draw circle at the blue line for valid vehicles\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"{v_type}-{vehicle_id}\", (cx, cy), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "            # ✅ Add to counter only once\n",
    "            if vehicle_id not in counter_down:\n",
    "                counter_down.append(vehicle_id)\n",
    "                vehicle_counts[v_type] += 1\n",
    "\n",
    "    # ✅ Draw reference lines\n",
    "    cv2.line(frame, (260, red_line_y), (724, red_line_y), (0, 0, 255), 3)\n",
    "    cv2.putText(frame, 'red line', (260, red_line_y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.line(frame, (200, blue_line_y), (779, blue_line_y), (255, 0, 0), 3)\n",
    "    cv2.putText(frame, 'blue line', (200, blue_line_y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # ✅ Display vehicle count\n",
    "    cv2.putText(frame, f\"Cars: {vehicle_counts['car']}  Buses: {vehicle_counts['bus']}  Trucks: {vehicle_counts['truck']}  Motorcycles: {vehicle_counts['motorcycle']}\",\n",
    "                (60, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"frames\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# ✅ Log the final vehicle count when the video ends\n",
    "timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "number_of_lanes=2\n",
    "# ✅ Calculate light duration using the given formula\n",
    "light_duration = ((vehicle_counts[\"car\"] * 2) + \n",
    "                  (vehicle_counts[\"motorcycle\"] * 1.8) + \n",
    "                  ((vehicle_counts[\"bus\"] + vehicle_counts[\"truck\"]) * 3.5)) / number_of_lanes\n",
    "with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([timestamp, vehicle_counts[\"car\"], vehicle_counts[\"bus\"], vehicle_counts[\"truck\"], vehicle_counts[\"motorcycle\"], round(light_duration, 2)])\n",
    "\n",
    "print(f\"✅ Final vehicle count saved to CSV: {vehicle_counts}, Light Duration: {round(light_duration, 2)} sec\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130954a1-e786-4a3e-bd8a-586e4f4e259a",
   "metadata": {},
   "source": [
    "# Noon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c624c33a-8c1c-4ee3-9d9b-ee4d2ba792cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 4 cars, 1 motorcycle, 1 bus, 2 trucks, 262.7ms\n",
      "Speed: 4.1ms preprocess, 262.7ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 1 motorcycle, 1 truck, 248.7ms\n",
      "Speed: 6.6ms preprocess, 248.7ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 1 motorcycle, 1 truck, 263.4ms\n",
      "Speed: 6.0ms preprocess, 263.4ms inference, 3.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 1 motorcycle, 1 truck, 282.8ms\n",
      "Speed: 8.5ms preprocess, 282.8ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 1 motorcycle, 1 truck, 263.7ms\n",
      "Speed: 5.2ms preprocess, 263.7ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 1 motorcycle, 1 truck, 241.0ms\n",
      "Speed: 6.4ms preprocess, 241.0ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 1 motorcycle, 1 truck, 242.7ms\n",
      "Speed: 5.1ms preprocess, 242.7ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 1 motorcycle, 1 truck, 262.2ms\n",
      "Speed: 4.8ms preprocess, 262.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 1 motorcycle, 1 truck, 240.2ms\n",
      "Speed: 5.4ms preprocess, 240.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 1 motorcycle, 1 truck, 236.2ms\n",
      "Speed: 5.5ms preprocess, 236.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 motorcycle, 1 truck, 257.1ms\n",
      "Speed: 4.9ms preprocess, 257.1ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 1 motorcycle, 2 trucks, 233.2ms\n",
      "Speed: 6.1ms preprocess, 233.2ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 motorcycle, 1 truck, 239.2ms\n",
      "Speed: 5.1ms preprocess, 239.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 1 motorcycle, 1 bus, 1 truck, 230.4ms\n",
      "Speed: 5.2ms preprocess, 230.4ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 1 motorcycle, 1 bus, 232.9ms\n",
      "Speed: 5.7ms preprocess, 232.9ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 bus, 235.8ms\n",
      "Speed: 5.2ms preprocess, 235.8ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 2 trucks, 235.6ms\n",
      "Speed: 5.6ms preprocess, 235.6ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 1 motorcycle, 1 truck, 230.4ms\n",
      "Speed: 5.1ms preprocess, 230.4ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 1 motorcycle, 1 truck, 249.5ms\n",
      "Speed: 5.3ms preprocess, 249.5ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 2 trucks, 240.2ms\n",
      "Speed: 5.5ms preprocess, 240.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 2 trucks, 233.0ms\n",
      "Speed: 7.1ms preprocess, 233.0ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 1 truck, 228.1ms\n",
      "Speed: 6.1ms preprocess, 228.1ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 truck, 231.6ms\n",
      "Speed: 7.4ms preprocess, 231.6ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 1 truck, 248.2ms\n",
      "Speed: 5.9ms preprocess, 248.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 2 trucks, 236.2ms\n",
      "Speed: 4.9ms preprocess, 236.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 6 cars, 2 trucks, 235.5ms\n",
      "Speed: 5.3ms preprocess, 235.5ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 7 cars, 2 trucks, 238.6ms\n",
      "Speed: 6.0ms preprocess, 238.6ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 cars, 2 trucks, 243.5ms\n",
      "Speed: 5.4ms preprocess, 243.5ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 2 trucks, 228.6ms\n",
      "Speed: 5.6ms preprocess, 228.6ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 4 cars, 2 trucks, 238.3ms\n",
      "Speed: 5.3ms preprocess, 238.3ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 4 cars, 2 trucks, 234.2ms\n",
      "Speed: 6.4ms preprocess, 234.2ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 3 cars, 2 trucks, 237.9ms\n",
      "Speed: 5.7ms preprocess, 237.9ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 2 trucks, 241.3ms\n",
      "Speed: 6.5ms preprocess, 241.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 2 trucks, 256.6ms\n",
      "Speed: 4.9ms preprocess, 256.6ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 2 trucks, 273.3ms\n",
      "Speed: 5.0ms preprocess, 273.3ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 2 trucks, 285.9ms\n",
      "Speed: 6.7ms preprocess, 285.9ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 2 trucks, 278.5ms\n",
      "Speed: 5.4ms preprocess, 278.5ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 2 trucks, 228.5ms\n",
      "Speed: 10.0ms preprocess, 228.5ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 1 motorcycle, 2 trucks, 230.2ms\n",
      "Speed: 5.1ms preprocess, 230.2ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 1 motorcycle, 2 trucks, 228.0ms\n",
      "Speed: 7.7ms preprocess, 228.0ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 1 motorcycle, 2 trucks, 234.6ms\n",
      "Speed: 8.3ms preprocess, 234.6ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 1 motorcycle, 2 trucks, 228.6ms\n",
      "Speed: 5.6ms preprocess, 228.6ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 1 motorcycle, 2 trucks, 226.9ms\n",
      "Speed: 5.4ms preprocess, 226.9ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 1 motorcycle, 1 bus, 2 trucks, 234.0ms\n",
      "Speed: 4.8ms preprocess, 234.0ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 2 motorcycles, 1 bus, 2 trucks, 233.3ms\n",
      "Speed: 5.0ms preprocess, 233.3ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 1 motorcycle, 2 trucks, 230.5ms\n",
      "Speed: 4.9ms preprocess, 230.5ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 1 motorcycle, 2 trucks, 229.9ms\n",
      "Speed: 5.3ms preprocess, 229.9ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 4 cars, 1 motorcycle, 2 trucks, 224.0ms\n",
      "Speed: 4.8ms preprocess, 224.0ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 persons, 3 cars, 1 motorcycle, 2 trucks, 264.2ms\n",
      "Speed: 4.6ms preprocess, 264.2ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 persons, 3 cars, 1 motorcycle, 2 buss, 1 truck, 232.3ms\n",
      "Speed: 4.5ms preprocess, 232.3ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 1 motorcycle, 1 bus, 224.9ms\n",
      "Speed: 5.5ms preprocess, 224.9ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 1 motorcycle, 233.3ms\n",
      "Speed: 4.7ms preprocess, 233.3ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 1 motorcycle, 242.0ms\n",
      "Speed: 4.1ms preprocess, 242.0ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 person, 4 cars, 2 motorcycles, 228.2ms\n",
      "Speed: 5.5ms preprocess, 228.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 2 motorcycles, 230.1ms\n",
      "Speed: 5.3ms preprocess, 230.1ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 2 motorcycles, 233.5ms\n",
      "Speed: 4.3ms preprocess, 233.5ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 cars, 1 motorcycle, 1 truck, 238.4ms\n",
      "Speed: 10.5ms preprocess, 238.4ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 1 motorcycle, 225.9ms\n",
      "Speed: 5.4ms preprocess, 225.9ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 1 motorcycle, 1 bus, 229.6ms\n",
      "Speed: 5.4ms preprocess, 229.6ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 cars, 1 motorcycle, 1 bus, 239.8ms\n",
      "Speed: 4.2ms preprocess, 239.8ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 1 motorcycle, 1 bus, 230.3ms\n",
      "Speed: 5.4ms preprocess, 230.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 1 motorcycle, 1 bus, 249.3ms\n",
      "Speed: 6.2ms preprocess, 249.3ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 cars, 1 motorcycle, 1 bus, 250.5ms\n",
      "Speed: 7.8ms preprocess, 250.5ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 1 motorcycle, 1 bus, 265.6ms\n",
      "Speed: 5.6ms preprocess, 265.6ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 cars, 1 motorcycle, 1 bus, 263.6ms\n",
      "Speed: 5.0ms preprocess, 263.6ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m1020\u001b[39m, \u001b[38;5;241m500\u001b[39m))\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# ✅ YOLO Detection\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m a \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     63\u001b[0m px \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(a)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\model.py:560\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:261\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 261\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:145\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    140\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    141\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    144\u001b[0m )\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\autobackend.py:557\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 557\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:114\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:132\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:153\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 153\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    154\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\modules\\block.py:238\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    239\u001b[0m     y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\modules\\conv.py:55\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution and activation without batch normalization.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from tracker import *\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ✅ Load YOLO model\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "# ✅ Object classes\n",
    "class_list = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "    \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "    \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\",\n",
    "    \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\n",
    "    \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
    "    \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
    "    \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"TV\", \"laptop\",\n",
    "    \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "    \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "# ✅ Initialize tracker\n",
    "tracker = Tracker()\n",
    "cap = cv2.VideoCapture('IMG_8979.MOV')\n",
    "\n",
    "# ✅ CSV File Setup\n",
    "csv_filename = \"12vehicles_summary.csv\"\n",
    "\n",
    "# ✅ Create CSV file if it doesn't exist\n",
    "if not os.path.exists(csv_filename):\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Timestamp\", \"Cars\", \"Buses\", \"Trucks\", \"Motorcycles\"])\n",
    "\n",
    "# ✅ Define reference lines\n",
    "red_line_y = 335\n",
    "blue_line_y = 355\n",
    "offset = 7  # Allow slight variation due to tracking jitter\n",
    "\n",
    "# ✅ Define vehicle classes to track\n",
    "vehicle_classes = [\"car\", \"bus\", \"truck\", \"motorcycle\"]\n",
    "vehicle_counts = {\"car\": 0, \"bus\": 0, \"truck\": 0, \"motorcycle\": 0}\n",
    "\n",
    "# ✅ Store vehicle IDs that crossed the red line first\n",
    "crossed_red = {}\n",
    "counter_down = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (1020, 500))\n",
    "\n",
    "    # ✅ YOLO Detection\n",
    "    results = model.predict(frame)\n",
    "    a = results[0].boxes.data.detach().cpu().numpy()\n",
    "    px = pd.DataFrame(a).astype(\"float\")\n",
    "\n",
    "    # ✅ Store detections\n",
    "    detections = []\n",
    "    detected_vehicles = {}\n",
    "\n",
    "    for _, row in px.iterrows():\n",
    "        x1, y1, x2, y2 = int(row[0]), int(row[1]), int(row[2]), int(row[3])\n",
    "        d = int(row[5])\n",
    "        vehicle_type = class_list[d]\n",
    "\n",
    "        if vehicle_type in vehicle_classes:\n",
    "            detections.append([x1, y1, x2, y2])\n",
    "            detected_vehicles[(x1, y1, x2, y2)] = vehicle_type\n",
    "\n",
    "    # ✅ Ensure 'detections' is not empty\n",
    "    bbox_id = tracker.update(np.array(detections)) if detections else []\n",
    "\n",
    "    # ✅ Process ALL detected vehicles\n",
    "    for bbox in bbox_id:\n",
    "        x3, y3, x4, y4, vehicle_id = bbox\n",
    "        cx, cy = (x3 + x4) // 2, (y3 + y4) // 2\n",
    "\n",
    "        vehicle_type = detected_vehicles.get((x3, y3, x4, y4), \"unknown\")\n",
    "\n",
    "        # ✅ Check if the vehicle crosses the RED line first\n",
    "        if red_line_y - offset < cy < red_line_y + offset:\n",
    "            crossed_red[vehicle_id] = (cy, vehicle_type)\n",
    "\n",
    "        # ✅ Only consider vehicles that crossed red before reaching blue\n",
    "        if vehicle_id in crossed_red and blue_line_y - offset < cy < blue_line_y + offset:\n",
    "            _, v_type = crossed_red[vehicle_id]\n",
    "\n",
    "            # ✅ Draw circle at the blue line for valid vehicles\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"{v_type}-{vehicle_id}\", (cx, cy), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "            # ✅ Add to counter only once\n",
    "            if vehicle_id not in counter_down:\n",
    "                counter_down.append(vehicle_id)\n",
    "                vehicle_counts[v_type] += 1\n",
    "\n",
    "    # ✅ Draw reference lines\n",
    "    cv2.line(frame, (500, red_line_y), (790, red_line_y), (0, 0, 255), 3)\n",
    "    cv2.putText(frame, 'red line', (500, red_line_y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.line(frame, (290, blue_line_y), (750, blue_line_y), (255, 0, 0), 3)\n",
    "    cv2.putText(frame, 'blue line', (290, blue_line_y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # ✅ Display vehicle count\n",
    "    cv2.putText(frame, f\"Cars: {vehicle_counts['car']}  Buses: {vehicle_counts['bus']}  Trucks: {vehicle_counts['truck']}  Motorcycles: {vehicle_counts['motorcycle']}\",\n",
    "                (60, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"frames\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# ✅ Log the final vehicle count when the video ends\n",
    "timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "number_of_lanes=2\n",
    "# ✅ Calculate light duration using the given formula\n",
    "light_duration = ((vehicle_counts[\"car\"] * 2) + \n",
    "                  (vehicle_counts[\"motorcycle\"] * 1.8) + \n",
    "                  ((vehicle_counts[\"bus\"] + vehicle_counts[\"truck\"]) * 3.5)) / number_of_lanes\n",
    "with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([timestamp, vehicle_counts[\"car\"], vehicle_counts[\"bus\"], vehicle_counts[\"truck\"], vehicle_counts[\"motorcycle\"], round(light_duration, 2)])\n",
    "\n",
    "print(f\"✅ Final vehicle count saved to CSV: {vehicle_counts}, Light Duration: {round(light_duration, 2)} sec\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55be6cc-2711-47c4-886b-6a91f12f747c",
   "metadata": {},
   "source": [
    "# Evening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825de941-349b-4dee-9872-35a387d0d931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 13 persons, 3 cars, 10 motorcycles, 1 truck, 1 backpack, 250.3ms\n",
      "Speed: 4.9ms preprocess, 250.3ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 persons, 3 cars, 11 motorcycles, 1 truck, 266.3ms\n",
      "Speed: 6.0ms preprocess, 266.3ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 persons, 3 cars, 12 motorcycles, 1 truck, 283.8ms\n",
      "Speed: 6.2ms preprocess, 283.8ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 14 persons, 3 cars, 12 motorcycles, 1 truck, 283.2ms\n",
      "Speed: 6.3ms preprocess, 283.2ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 14 persons, 3 cars, 11 motorcycles, 1 truck, 311.1ms\n",
      "Speed: 12.3ms preprocess, 311.1ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 11 persons, 3 cars, 12 motorcycles, 1 truck, 230.4ms\n",
      "Speed: 5.7ms preprocess, 230.4ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 3 cars, 11 motorcycles, 1 truck, 237.1ms\n",
      "Speed: 5.4ms preprocess, 237.1ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 persons, 3 cars, 12 motorcycles, 1 truck, 224.5ms\n",
      "Speed: 5.0ms preprocess, 224.5ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 4 cars, 8 motorcycles, 1 truck, 230.9ms\n",
      "Speed: 5.2ms preprocess, 230.9ms inference, 3.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 13 persons, 4 cars, 9 motorcycles, 220.9ms\n",
      "Speed: 4.5ms preprocess, 220.9ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 15 persons, 2 cars, 9 motorcycles, 238.3ms\n",
      "Speed: 4.8ms preprocess, 238.3ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 3 cars, 10 motorcycles, 225.6ms\n",
      "Speed: 4.6ms preprocess, 225.6ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 persons, 3 cars, 12 motorcycles, 227.9ms\n",
      "Speed: 5.2ms preprocess, 227.9ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 3 cars, 12 motorcycles, 221.9ms\n",
      "Speed: 4.7ms preprocess, 221.9ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 persons, 3 cars, 12 motorcycles, 218.2ms\n",
      "Speed: 4.7ms preprocess, 218.2ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 persons, 4 cars, 12 motorcycles, 230.7ms\n",
      "Speed: 4.8ms preprocess, 230.7ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 11 persons, 4 cars, 12 motorcycles, 266.3ms\n",
      "Speed: 4.5ms preprocess, 266.3ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 13 persons, 5 cars, 14 motorcycles, 225.1ms\n",
      "Speed: 5.7ms preprocess, 225.1ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 13 persons, 3 cars, 15 motorcycles, 232.9ms\n",
      "Speed: 5.9ms preprocess, 232.9ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 persons, 5 cars, 14 motorcycles, 1 truck, 229.0ms\n",
      "Speed: 4.5ms preprocess, 229.0ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 5 cars, 15 motorcycles, 1 truck, 233.2ms\n",
      "Speed: 5.4ms preprocess, 233.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 10 persons, 6 cars, 13 motorcycles, 1 truck, 260.2ms\n",
      "Speed: 5.8ms preprocess, 260.2ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 13 persons, 5 cars, 12 motorcycles, 1 truck, 256.9ms\n",
      "Speed: 6.1ms preprocess, 256.9ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 11 persons, 5 cars, 13 motorcycles, 1 truck, 256.5ms\n",
      "Speed: 5.6ms preprocess, 256.5ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 persons, 5 cars, 12 motorcycles, 247.1ms\n",
      "Speed: 5.7ms preprocess, 247.1ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 13 persons, 5 cars, 12 motorcycles, 535.0ms\n",
      "Speed: 5.3ms preprocess, 535.0ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 persons, 5 cars, 11 motorcycles, 280.3ms\n",
      "Speed: 6.3ms preprocess, 280.3ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 14 persons, 6 cars, 13 motorcycles, 252.8ms\n",
      "Speed: 5.3ms preprocess, 252.8ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 15 persons, 6 cars, 12 motorcycles, 260.0ms\n",
      "Speed: 4.9ms preprocess, 260.0ms inference, 3.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 16 persons, 4 cars, 11 motorcycles, 342.7ms\n",
      "Speed: 16.8ms preprocess, 342.7ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 persons, 4 cars, 9 motorcycles, 240.0ms\n",
      "Speed: 4.3ms preprocess, 240.0ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 11 persons, 4 cars, 10 motorcycles, 258.0ms\n",
      "Speed: 7.4ms preprocess, 258.0ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 10 persons, 4 cars, 10 motorcycles, 234.1ms\n",
      "Speed: 5.3ms preprocess, 234.1ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 persons, 3 cars, 9 motorcycles, 230.2ms\n",
      "Speed: 5.2ms preprocess, 230.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 13 persons, 3 cars, 8 motorcycles, 232.4ms\n",
      "Speed: 5.0ms preprocess, 232.4ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 persons, 4 cars, 10 motorcycles, 233.2ms\n",
      "Speed: 4.6ms preprocess, 233.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 11 persons, 4 cars, 9 motorcycles, 1 truck, 230.5ms\n",
      "Speed: 5.7ms preprocess, 230.5ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 11 persons, 5 cars, 11 motorcycles, 227.2ms\n",
      "Speed: 5.6ms preprocess, 227.2ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 10 persons, 6 cars, 10 motorcycles, 241.5ms\n",
      "Speed: 5.5ms preprocess, 241.5ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 persons, 5 cars, 10 motorcycles, 1 bus, 223.9ms\n",
      "Speed: 6.2ms preprocess, 223.9ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 5 cars, 11 motorcycles, 219.7ms\n",
      "Speed: 8.7ms preprocess, 219.7ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 persons, 5 cars, 12 motorcycles, 1 bus, 227.8ms\n",
      "Speed: 6.4ms preprocess, 227.8ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 persons, 5 cars, 11 motorcycles, 1 bus, 227.3ms\n",
      "Speed: 5.7ms preprocess, 227.3ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 persons, 5 cars, 11 motorcycles, 1 bus, 230.2ms\n",
      "Speed: 6.7ms preprocess, 230.2ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 5 cars, 11 motorcycles, 1 bus, 222.6ms\n",
      "Speed: 6.5ms preprocess, 222.6ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 10 persons, 6 cars, 7 motorcycles, 1 bus, 231.6ms\n",
      "Speed: 4.9ms preprocess, 231.6ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 6 cars, 9 motorcycles, 1 bus, 240.7ms\n",
      "Speed: 7.6ms preprocess, 240.7ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 13 persons, 6 cars, 8 motorcycles, 1 bus, 229.4ms\n",
      "Speed: 5.6ms preprocess, 229.4ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 persons, 8 cars, 8 motorcycles, 1 bus, 227.3ms\n",
      "Speed: 6.6ms preprocess, 227.3ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 persons, 7 cars, 6 motorcycles, 1 bus, 236.2ms\n",
      "Speed: 5.2ms preprocess, 236.2ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 15 persons, 8 cars, 7 motorcycles, 1 bus, 1 truck, 230.2ms\n",
      "Speed: 6.7ms preprocess, 230.2ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 13 persons, 7 cars, 6 motorcycles, 1 bus, 249.6ms\n",
      "Speed: 7.6ms preprocess, 249.6ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 10 persons, 9 cars, 6 motorcycles, 1 bus, 250.8ms\n",
      "Speed: 8.3ms preprocess, 250.8ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 14 persons, 7 cars, 6 motorcycles, 284.8ms\n",
      "Speed: 7.2ms preprocess, 284.8ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 15 persons, 8 cars, 7 motorcycles, 1 bus, 1 truck, 242.8ms\n",
      "Speed: 5.4ms preprocess, 242.8ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 12 persons, 7 cars, 5 motorcycles, 1 bus, 246.8ms\n",
      "Speed: 5.6ms preprocess, 246.8ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 13 persons, 9 cars, 4 motorcycles, 1 bus, 386.5ms\n",
      "Speed: 6.9ms preprocess, 386.5ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 10 persons, 9 cars, 4 motorcycles, 1 bus, 1 truck, 414.9ms\n",
      "Speed: 8.3ms preprocess, 414.9ms inference, 6.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 10 persons, 6 cars, 3 motorcycles, 1 bus, 1 truck, 376.9ms\n",
      "Speed: 7.9ms preprocess, 376.9ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 persons, 5 cars, 3 motorcycles, 1 bus, 1 truck, 363.4ms\n",
      "Speed: 6.1ms preprocess, 363.4ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 persons, 5 cars, 4 motorcycles, 1 bus, 1 truck, 396.8ms\n",
      "Speed: 7.3ms preprocess, 396.8ms inference, 4.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 4 cars, 5 motorcycles, 1 bus, 1 truck, 552.0ms\n",
      "Speed: 15.2ms preprocess, 552.0ms inference, 3.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 4 cars, 5 motorcycles, 1 truck, 327.0ms\n",
      "Speed: 6.4ms preprocess, 327.0ms inference, 3.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 persons, 6 cars, 6 motorcycles, 1 truck, 387.9ms\n",
      "Speed: 6.9ms preprocess, 387.9ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 4 cars, 6 motorcycles, 1 truck, 451.8ms\n",
      "Speed: 7.6ms preprocess, 451.8ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 persons, 4 cars, 6 motorcycles, 1 truck, 355.0ms\n",
      "Speed: 9.1ms preprocess, 355.0ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 8 persons, 4 cars, 6 motorcycles, 1 truck, 258.9ms\n",
      "Speed: 5.8ms preprocess, 258.9ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 3 cars, 5 motorcycles, 1 truck, 267.2ms\n",
      "Speed: 8.2ms preprocess, 267.2ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 persons, 3 cars, 5 motorcycles, 1 truck, 271.1ms\n",
      "Speed: 6.0ms preprocess, 271.1ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 9 persons, 3 cars, 5 motorcycles, 1 truck, 251.4ms\n",
      "Speed: 5.3ms preprocess, 251.4ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m1020\u001b[39m, \u001b[38;5;241m500\u001b[39m))\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# ✅ YOLO Detection\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m a \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     63\u001b[0m px \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(a)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\model.py:560\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:261\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 261\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:145\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    140\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    141\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    144\u001b[0m )\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\autobackend.py:557\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 557\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:114\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:132\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:153\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 153\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    154\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\modules\\conv.py:55\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution and activation without batch normalization.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from tracker import *\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ✅ Load YOLO model\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "# ✅ Object classes\n",
    "class_list = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "    \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "    \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\",\n",
    "    \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\n",
    "    \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
    "    \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
    "    \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"TV\", \"laptop\",\n",
    "    \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "    \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "# ✅ Initialize tracker\n",
    "tracker = Tracker()\n",
    "cap = cv2.VideoCapture('bus_stand_eve.mp4')\n",
    "\n",
    "# ✅ CSV File Setup\n",
    "csv_filename = \"12vehicles_summary.csv\"\n",
    "\n",
    "# ✅ Create CSV file if it doesn't exist\n",
    "if not os.path.exists(csv_filename):\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Timestamp\", \"Cars\", \"Buses\", \"Trucks\", \"Motorcycles\"])\n",
    "\n",
    "# ✅ Define reference lines\n",
    "red_line_y = 335\n",
    "blue_line_y = 355\n",
    "offset = 7  # Allow slight variation due to tracking jitter\n",
    "\n",
    "# ✅ Define vehicle classes to track\n",
    "vehicle_classes = [\"car\", \"bus\", \"truck\", \"motorcycle\"]\n",
    "vehicle_counts = {\"car\": 0, \"bus\": 0, \"truck\": 0, \"motorcycle\": 0}\n",
    "\n",
    "# ✅ Store vehicle IDs that crossed the red line first\n",
    "crossed_red = {}\n",
    "counter_down = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (1020, 500))\n",
    "\n",
    "    # ✅ YOLO Detection\n",
    "    results = model.predict(frame)\n",
    "    a = results[0].boxes.data.detach().cpu().numpy()\n",
    "    px = pd.DataFrame(a).astype(\"float\")\n",
    "\n",
    "    # ✅ Store detections\n",
    "    detections = []\n",
    "    detected_vehicles = {}\n",
    "\n",
    "    for _, row in px.iterrows():\n",
    "        x1, y1, x2, y2 = int(row[0]), int(row[1]), int(row[2]), int(row[3])\n",
    "        d = int(row[5])\n",
    "        vehicle_type = class_list[d]\n",
    "\n",
    "        if vehicle_type in vehicle_classes:\n",
    "            detections.append([x1, y1, x2, y2])\n",
    "            detected_vehicles[(x1, y1, x2, y2)] = vehicle_type\n",
    "\n",
    "    # ✅ Ensure 'detections' is not empty\n",
    "    bbox_id = tracker.update(np.array(detections)) if detections else []\n",
    "\n",
    "    # ✅ Process ALL detected vehicles\n",
    "    for bbox in bbox_id:\n",
    "        x3, y3, x4, y4, vehicle_id = bbox\n",
    "        cx, cy = (x3 + x4) // 2, (y3 + y4) // 2\n",
    "\n",
    "        vehicle_type = detected_vehicles.get((x3, y3, x4, y4), \"unknown\")\n",
    "\n",
    "        # ✅ Check if the vehicle crosses the RED line first\n",
    "        if red_line_y - offset < cy < red_line_y + offset:\n",
    "            crossed_red[vehicle_id] = (cy, vehicle_type)\n",
    "\n",
    "        # ✅ Only consider vehicles that crossed red before reaching blue\n",
    "        if vehicle_id in crossed_red and blue_line_y - offset < cy < blue_line_y + offset:\n",
    "            _, v_type = crossed_red[vehicle_id]\n",
    "\n",
    "            # ✅ Draw circle at the blue line for valid vehicles\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"{v_type}-{vehicle_id}\", (cx, cy), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "            # ✅ Add to counter only once\n",
    "            if vehicle_id not in counter_down:\n",
    "                counter_down.append(vehicle_id)\n",
    "                vehicle_counts[v_type] += 1\n",
    "\n",
    "    # ✅ Draw reference lines\n",
    "    cv2.line(frame, (350, red_line_y), (840, red_line_y), (0, 0, 255), 3)\n",
    "    cv2.putText(frame, 'red line', (350, red_line_y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.line(frame, (240, blue_line_y), (840, blue_line_y), (255, 0, 0), 3)\n",
    "    cv2.putText(frame, 'blue line', (240, blue_line_y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # ✅ Display vehicle count\n",
    "    cv2.putText(frame, f\"Cars: {vehicle_counts['car']}  Buses: {vehicle_counts['bus']}  Trucks: {vehicle_counts['truck']}  Motorcycles: {vehicle_counts['motorcycle']}\",\n",
    "                (60, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"frames\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# ✅ Log the final vehicle count when the video ends\n",
    "timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "number_of_lanes=2\n",
    "# ✅ Calculate light duration using the given formula\n",
    "light_duration = ((vehicle_counts[\"car\"] * 2) + \n",
    "                  (vehicle_counts[\"motorcycle\"] * 1.8) + \n",
    "                  ((vehicle_counts[\"bus\"] + vehicle_counts[\"truck\"]) * 3.5)) / number_of_lanes\n",
    "with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([timestamp, vehicle_counts[\"car\"], vehicle_counts[\"bus\"], vehicle_counts[\"truck\"], vehicle_counts[\"motorcycle\"], round(light_duration, 2)])\n",
    "\n",
    "print(f\"✅ Final vehicle count saved to CSV: {vehicle_counts}, Light Duration: {round(light_duration, 2)} sec\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c68d8c6-a090-4718-b280-52071f67592f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
